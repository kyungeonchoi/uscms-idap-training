{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6e0dee-d651-4dbc-9e6a-66901658e28e",
   "metadata": {},
   "source": [
    "![title](figures/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce9405-b23a-4b81-99ef-1688ab3c5d37",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "IRIS-HEP (Institute for Research and Innovation in Software for High Energy Physics) is a software institute comprising 16 U.S. institutions, primarily from US CMS and US ATLAS institutes, that develops state-of-the-art software cyberinfrastructure for the High-Luminosity Large Hadron Collider at CERN.\n",
    "\n",
    "IRIS-HEP has various components: Analysis Systems, Data Organization, Management and Access, Innovative Algorithms, Scalable Systems Laboratory, etc.\n",
    "\n",
    "Many of these components are designed to improve user analysis workflow with the following keywords: Python ecosystem, industry/ML-friendly tools, experiment-agnostic, better UI/UX, etc.\n",
    "\n",
    "This notebook showcases most of these components through an example ttbar analysis using CMS Open Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a916b4-25db-44a6-a69f-176045ef1cbf",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "\n",
    "- This notebook is primarily based on the materials shown at the latest US CMS/IRIS-HEP Analysis Software Training 2025 on May 19-20, 2025 (https://indico.cern.ch/event/1509580/)\n",
    "- Since I'm not a member of CMS, experiment specific details might not be correct :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# Analysis Grand Challenge: CMS Open Data $t\\bar{t}$\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c03fef-22dc-4b52-8b4f-9583f423c59e",
   "metadata": {},
   "source": [
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "This is a **technical demonstration**. We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful. This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice. If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8537f4d-46b7-4f32-9c78-a3ed9705597d",
   "metadata": {},
   "source": [
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](figures/pipe.001.jpeg)\n",
    "\n",
    "This version also includes the Combine tool to perform model building ans statistical inference. Despite not strictly being part of the ecosystem, it is widely used in the CMS collaboration, so in this tutorial we will see an example of statistical inference performed with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1dd011-9b38-4335-9103-2398eff7ac30",
   "metadata": {},
   "source": [
    "# Columnar analysis and Awkward Array\n",
    "\n",
    "\"Columnar\" is a term that tends to get overloaded, and I’ve used it in two distinct ways:\n",
    "\n",
    "- Data layout: organizing data in memory or on disk to enable faster, selective readout. (In fact, some types of TTree data have been stored this way since 1995.)\n",
    "- Array-oriented computation: performing operations directly on entire arrays of data, rather than processing one value at a time in a loop. In other words — no for loops!\n",
    "\n",
    "Of these, only the second meaning directly affects physicists writing analysis code.\n",
    "\n",
    "That’s why I prefer to call it \"array-oriented programming\" — it’s a programming paradigm, much like \"imperative\" or \"functional,\" describing how you think about and organize your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda6ff6-b6cd-4a34-8f47-e6ab58fe55d0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Imperative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55c6ad-a128-4f94-b3e2-7649894fe725",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "result = []\n",
    "for x in original:\n",
    "    result.append(x**2)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6694ca-511f-458e-972a-8b2012868af4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Functional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434277ba-4935-4ee2-a792-ffe248620a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "result = [x**2 for x in original]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f4ff0-eff7-47f1-80be-90dde6568be5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Array-oriented**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274bee2e-5e4c-4e7f-a553-25ef949f9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e754fe3-485a-4374-8b18-600ce0e3f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "result = original**2\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9234a10-298f-44b9-b747-8059400bd044",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "There are two fundamental concepts you need to understand in the array programming: **axis** and **slice**\n",
    "\n",
    "Let's open a ttbar MC NanoAOD file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da366d3-1480-4582-9f01-70a03e531da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttbar_file = \"ttbar.root\"\n",
    "# ttbar_file = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/\"\\\n",
    "#     \"TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_\"\\\n",
    "#     \"mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50aaad-c993-4be2-9787-d8b861503ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e367c-aa21-4f21-886e-e1f9cd57b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "events = NanoEventsFactory.from_root({ttbar_file: \"Events\"}, schemaclass=NanoAODSchema, mode=\"virtual\").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb6ca2-d3a8-4370-a0bd-400124658729",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's read Electron pt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db227f64-14cd-4e1d-ba90-c32b9d604f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ak.materialize(events.Electron.pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ec2be-32e6-4226-8f11-db592599b5bc",
   "metadata": {},
   "source": [
    "Electron pt is represented by a 2D array with variable-length nested array. \n",
    "\n",
    "Almost always, `axis=0` is the *events* axis (collision events) and `axis=1` is the *particle* axis (electrons, muons, jets, ...)\n",
    "\n",
    "Let's check the number of events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63cc35-f61f-4a5c-bc29-8adcc4ce841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.num(events.Electron, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba91d9-b2f8-4d72-8693-7133cda26b01",
   "metadata": {},
   "source": [
    "Now check how many electrons in each event by passing `axis=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20073224-3550-448b-adaa-53c0594d5603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ak.num(events.Electron, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928f6ae-3cce-4bd8-ab44-e6658e1e073a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Slicing (or filtering) of an array requires a boolean array of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc574129-7fff-42ff-9c10-d94d31aa7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_mask = events.Electron.pt > 50\n",
    "boolean_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ac15b-025b-405d-8217-95df24c5d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electron.pt[boolean_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42829ced-6e04-4d82-a984-0c33b1ea32c0",
   "metadata": {},
   "source": [
    "Note that the slicing affects only `axis=1` (particle axis) and not `axis=0` (event axis). The array still has 225k events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1984b-46df-4124-93f9-b5dc41848db5",
   "metadata": {},
   "source": [
    "#### Let's have a look into a practical example\n",
    "\n",
    "<img src=\"figures/ttbar.png\" width=\"400\">\n",
    "\n",
    "- Basic kinematic selections on electron, muon and jets\n",
    "- Select events with 1 lepton\n",
    "- Select events with at least 4 jets\n",
    "- Select events with at least 2 b-tagged jets\n",
    "- Construct tri-jet objects from all jets in each event\n",
    "- Select tri-jet candidates with at least one b-tagged jet\n",
    "- Return mass of a tri-jet candidate with the largest pT\n",
    "\n",
    "Thus, poor man's top mass reconstruction :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[(events.Jet.pt > 25) & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return trijet_mass\n",
    "    # return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc04ec-5cd7-4d79-9d19-3c1e148e3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reconstructed_top_mass = calculate_trijet_mass(events)\n",
    "reconstructed_top_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5620a4-b058-4f66-9015-8d4b24c56118",
   "metadata": {},
   "source": [
    "What we have here is the 1D array of reconstructed top mass of 38k events (out of 225k events). In terms of data size, 553MB to 458kB.\n",
    "\n",
    "[Awkward array API documentation](https://awkward-array.org/doc/main/reference/index.html)\n",
    "\n",
    "Complete columnar/awkward notebook can be found [here](https://github.com/iris-hep/uscms-idap-training/tree/main/columnar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51533a-3a07-4f5c-b868-917d5c2bc081",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Data visualization\n",
    "\n",
    "`hist` package for histogramming (including manipulations) and `mplhep` for plotting\n",
    "\n",
    "#### hist\n",
    "\n",
    "- Python go to one-stop for histogramming\n",
    "- Extends boost-histogram (Python binding for C++ `Boost::Histogram` library — *FAST*)\n",
    "- Shortcuts for convenience and interactive plotting/fitting\n",
    "\n",
    "#### mplhep\n",
    "\n",
    "- Built on top of `matplotlib`\n",
    "- Extends functionality to easily plot histograms from various inputs\n",
    "- Holds style sheets for easy experiment specific style application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876c4c1-d343-48fb-9eb1-f0f617cefa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf22e0-3119-4043-9b0d-75d010ccae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_reco_mtop = hist.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(ak.flatten(reconstructed_top_mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda372cf-241d-4dd5-a364-7ad4ea5b58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hist_reco_mtop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "artists = hist_reco_mtop.plot()\n",
    "artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(\"figures/trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb4a1c-32ec-4e6c-a9d1-097340e0d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep\n",
    "\n",
    "mplhep.style.use([mplhep.style.CMS, {\"figure.figsize\": (8, 6)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502eba5-1399-4755-b3d8-aeec61b74d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mplhep.histplot(H=hist_reco_mtop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d76e5d-90ff-4253-99cb-e3d3adb1e300",
   "metadata": {},
   "source": [
    "More examples can be found [here](https://github.com/iris-hep/uscms-idap-training/blob/main/mplhep/hist_mplhep_basics.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc2f25-184e-4f3e-bcb8-5ec167d5f5da",
   "metadata": {},
   "source": [
    "# coffea columnar analysis framework\n",
    "\n",
    "#### Why do we need coffea?\n",
    "`Uproot` + `Awkward` combination happily performs complex analysis, but we ran on a single ROOT file. Real world implementations involve many more files with all different metadata of each, and one needs to distribute computations.\n",
    "\n",
    "coffea analysis framework is a wrapper package around `uproot` and `awkward`, and it provides many convenient features (especially for CMS users!):\n",
    "- Schema class: rich interpretation of TBranches\n",
    "- CMS high-level tools: `coffea.jetmet_tools`, `coffea.btag_tools`, etc\n",
    "- Distributed computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.processor import ProcessorABC, Runner, DaskExecutor, FuturesExecutor\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import correctionlib\n",
    "\n",
    "import utils\n",
    "from utils.systematics import rand_gauss\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='coffea')\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module='coffea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward` and `hist` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(elecs, muons, jets):\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "class create_histograms(ProcessorABC):\n",
    "    # create histograms with observables\n",
    "    def process(self, events):\n",
    "        hist_4j1b = (\n",
    "            hist.Hist.new.Reg(11, 110, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "    \n",
    "        hist_4j2b = (\n",
    "            hist.Hist.new.Reg(11, 110, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "    \n",
    "        hist_dict = {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}\n",
    "    \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        #process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "    \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "    \n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = rand_gauss(events.Jet.pt)\n",
    "    \n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "        \n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "        \n",
    "        for syst_var in syst_variations:\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "    \n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "        \n",
    "            elecs, muons, jets = object_selection(elecs, muons, jets)\n",
    "    \n",
    "            # region selection\n",
    "            selections = region_selection(elecs, muons, jets)\n",
    "    \n",
    "            for region in hist_dict:\n",
    "                selection = selections.all(region)\n",
    "                region_jets = jets[selection]\n",
    "                region_weights = ak.ones_like(ak.num(region_jets, axis=1)) * xsec_weight\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "                elif region == \"4j2b\":\n",
    "                    observable = calculate_m_reco_top(region_jets)\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        if syst_var == \"scale_var\":\n",
    "                            wgt_variation = cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:, 0])\n",
    "                        elif syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])\n",
    "                            wgt_variation = cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        hist_dict[region].fill(\n",
    "                            observable,\n",
    "                            process=process,\n",
    "                            variation=syst_var_name,\n",
    "                            weight=region_weights * wgt_variation,\n",
    "                        )\n",
    "                else:\n",
    "                    if variation != \"nominal\":\n",
    "                        syst_var_name = variation\n",
    "                    hist_dict[region].fill(\n",
    "                        observable,\n",
    "                        process=process,\n",
    "                        variation=syst_var_name,\n",
    "                        weight=region_weights,\n",
    "                    )\n",
    "    \n",
    "        return {events.metadata[\"dataset\"]: hist_dict}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need. More information on how the dataset was prepared can be found [here](https://github.com/iris-hep/analysis-grand-challenge/blob/main/analyses/cms-open-data-ttbar/ttbar_analysis_pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd7ce1-e630-4ea8-9c1a-20eb172374f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cef0f-c923-4b80-81aa-1226fc77a623",
   "metadata": {},
   "source": [
    "<img src=\"figures/Single_quark_process_diagrams.png\" width=\"800\">\n",
    "\n",
    "The diagrams shows single top production: (a) t-channel, (b) Wt-channel, (c) s-channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d04a-98b9-4222-9c1f-acaa3db882a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset['ttbar__nominal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae20ef3-a5de-4ea5-bfc7-3c873be3b94f",
   "metadata": {},
   "source": [
    "# Distributed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4626d-35b9-4ac2-b890-ee3b11e7b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e74b9e-1331-467e-8f55-88619798ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cf57e-8ffc-4616-9c01-645205e392d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc4d58-c7a7-4e1b-9c3d-ad36af88237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Runner\n",
    "run = Runner(\n",
    "    executor=DaskExecutor(client=client, compression=None),\n",
    "    # executor=FuturesExecutor(workers=4, compression=None),\n",
    "    chunksize=250_000,\n",
    "    skipbadfiles=True,\n",
    "    schema=NanoAODSchema,\n",
    "    savemetrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f40952-2973-44f6-aba1-84d63e9d2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples = run.preprocess(fileset, treename=\"Events\") # treename not needed with coffea master branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "tmp, report = run(samples, processor_instance=create_histograms())\n",
    "# sort the key order to be the same as the initial fileset\n",
    "out = {key: tmp[key] for key in fileset}\n",
    "sorted(report[\"columns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681750bd-5e4e-420b-8fb0-34efa848cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mplhep.style.use([mplhep.style.CMS, {\"figure.figsize\": (8, 6)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b509a-fa58-411b-9247-a175484a3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_histogram_4j1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2129849-786c-4449-8f48-487e79705c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump for stats inference with also pseudodata\n",
    "for region, histogram in [(\"bin4j1b\", full_histogram_4j1b), (\"bin4j2b\", full_histogram_4j2b)]:\n",
    "    utils.file_output.save_histograms(histogram, f\"all_histograms_fps{N_FILES_MAX_PER_SAMPLE}_{region}.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fac7ed-1b8a-4950-8804-abf445951021",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = ['wjets', 'single_top_tW', 'single_top_t_chan', 'ttbar']\n",
    "mplhep.histplot(\n",
    "    H=[full_histogram_4j1b[120j::hist.rebin(1), process, \"nominal\"] for process in processes],\n",
    "    stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\",\n",
    "    label=processes\n",
    ");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(1), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "# fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bf54a-98bd-4e11-8d38-516ed915a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "ttbar_label = 'ttbar'\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$H_T$ [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09d371-f43b-4288-8a6c-a6419235141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "full_histogram_4j2b[:, ttbar_label, \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "full_histogram_4j2b[:, ttbar_label, \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "full_histogram_4j2b[:, ttbar_label, \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc45b3-7ced-4231-9be1-49f706bce758",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# ServiceX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9d0d9-2325-402b-bfdc-cfa0567587be",
   "metadata": {},
   "source": [
    "<img src=\"figures/ServiceXDiagram2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15bb7f-5fd8-495d-8258-3479e11140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fa262-c8da-430a-87ad-101c6532221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "branches_in_need = report[\"columns\"] + [\"run\", \"luminosityBlock\", \"event\", \"Electron_phi\", \"Muon_phi\"]\n",
    "sorted(branches_in_need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade673b6-0a90-48e6-9d8d-5d673fea0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicex import query, dataset, deliver\n",
    "\n",
    "query = query.UprootRaw(\n",
    "                [\n",
    "                    {\n",
    "                        \"treename\": \"Events\",\n",
    "                        \"filter_name\": branches_in_need,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "spec = {\n",
    "    \"Sample\": [        \n",
    "            {\n",
    "                \"Name\": process,\n",
    "                \"Dataset\": dataset.FileList(list(fileset[process]['files'].keys())[0]),\n",
    "                \"Query\": query,\n",
    "            }\n",
    "            for process in fileset.keys()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# One can also apply event selections\n",
    "# \"cut\": \"((count_nonzero((Electron_pt > 30) & (abs(Electron_eta) < 2.1) & (Electron_cutBased == 4) & (Electron_sip3d < 4), axis=1)\" \\\n",
    "#       \"+ count_nonzero((Muon_pt > 30) & (abs(Muon_eta) < 2.1) & (Muon_tightId) & (Muon_pfRelIso04_all < 0.15), axis=1)) == 1)\" \\\n",
    "#       \"& (count_nonzero((Jet_pt > 25) & (abs(Jet_eta) < 2.4) & (Jet_jetId == 6), axis=1) >= 4)\" \\\n",
    "#       \"& (count_nonzero((Jet_pt > 25) & (abs(Jet_eta) < 2.4) & (Jet_jetId == 6) & (Jet_btagCSVV2 > 0.5), axis=1) >= 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f7330-52ac-4449-8aea-16adeeca8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "servicex_delivery = deliver(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ed14b-b062-4e41-b407-149a8e102ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicex_delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89518258-6bda-4714-aa25-f46a72b918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset_sx = fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cab88d-69cf-4c5c-8b76-f1e44707a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for process in fileset_sx.keys():\n",
    "    fileset_sx[process]['files'] = {name:'Events' for name in servicex_delivery[process]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3d1b4-491f-41a0-8824-64b1d7223a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples_sx = run.preprocess(fileset_sx, treename=\"Events\") # treename not needed with coffea master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81edc6-5ecd-4fce-9a30-d9b7f365f503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "tmp_sx, report = run(samples_sx, processor_instance=create_histograms())\n",
    "# sort the key order to be the same as the initial fileset\n",
    "out_sx = {key: tmp_sx[key] for key in fileset_sx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca6a79-d65a-4af0-8921-86b52f9a5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_histogram_4j2b_sx = sum([v[\"4j2b\"] for v in out_sx.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f4cdc-206c-4101-8d02-1cb003c7bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = ['wjets', 'single_top_tW', 'single_top_t_chan', 'ttbar']\n",
    "mplhep.histplot(\n",
    "    H=[full_histogram_4j2b_sx[120j::hist.rebin(1), process, \"nominal\"] for process in processes],\n",
    "    stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\",\n",
    "    label=processes\n",
    ");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea0e7f-bf17-479b-ae49-5d0ee64751bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8253a1-b1de-4d29-bca7-e85dd748f238",
   "metadata": {},
   "source": [
    "# Statistical tools\n",
    "\n",
    "#### CMS Combine\n",
    "\n",
    "Histograms are interfaced to the CMS Combine tool for staticstical analysis at the US CMS/IRIS-HEP Analysis Software Training event. [Here](https://docs.google.com/presentation/d/1KoLQ4rvcVMJcr9ErGBNywne6d5t4CCtiM2_fIRyLDfE/edit?slide=id.p#slide=id.p) is the instruction to run inside coffea-casa instance at UNL.\n",
    "\n",
    "#### pyhf / cabinetry\n",
    "\n",
    "`pyhf`/`cabinetry` python packages are statistical analysis tools for building and steering binned template fits.\n",
    "Latest tutorial materials can be found [here](https://github.com/iris-hep/us-atlas-idap-training-2025/tree/main/stats-tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f570d-160f-4727-a7aa-f733da05ac8c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Introduced latest IRIS-HEP Analysis Software stack\n",
    "- Many independent components that can be implemented in any analysis workflow\n",
    "- [This](https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar) notebook includes ML training/inference pipeline as shown below\n",
    "\n",
    "![ecosystem visualization](figures/ecosystem.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe5ae8-dda2-4cb0-8abb-3e40cc79c3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
